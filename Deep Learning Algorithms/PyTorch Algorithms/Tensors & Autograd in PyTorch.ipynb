{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Fundamentals: Tensors & Autograd\n",
    "\n",
    "Welcome to the Engine Room. \n",
    "PyTorch has two superpowers:\n",
    "1.  It can run on **GPUs** (Graphics Cards).\n",
    "2.  It can do **Automatic Differentiation** (Autograd).\n",
    "\n",
    "### Tensors vs NumPy Arrays\n",
    "A `Tensor` is just a multi-dimensional matrix. It is almost identical to a `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu126\n",
      "Tensor from list:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Tensor from NumPy:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# From List\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor from list:\\n{x_data}\")\n",
    "\n",
    "# From NumPy\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor from NumPy:\\n{x_np}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GPU Check\n",
    "Deep Learning involves multiplying massive matrices. CPUs are smart but slow (few cores). GPUs are dumb but fast (thousands of cores).\n",
    "\n",
    "In PyTorch, we explicitly move data to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tensor on device: tensor([[1, 2],\n",
      "        [3, 4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move tensor to device\n",
    "x_gpu = x_data.to(device)\n",
    "print(f\"Tensor on device: {x_gpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd: The Magic Trick\n",
    "How do Neural Networks learn? **Calculus (Chain Rule)**.\n",
    "They need to know: *\"If I increase this weight by 0.001, how much will the Error go down?\"*\n",
    "\n",
    "Manually coding derivatives is a nightmare. PyTorch does it for you.\n",
    "\n",
    "Let's try: $y = x^3 + 5$\n",
    "We know the derivative is $\\frac{dy}{dx} = 3x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of equation (y): 13.0\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor, but tell PyTorch to WATCH it\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Forward Pass\n",
    "y = x**3 + 5\n",
    "\n",
    "print(f\"Result of equation (y): {y}\") # Should be 2^3 + 5 = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask for the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (dy/dx): 12.0\n"
     ]
    }
   ],
   "source": [
    "# Backward Pass (Calculate derivatives)\n",
    "y.backward()\n",
    "\n",
    "# Check gradient of x\n",
    "print(f\"Gradient (dy/dx): {x.grad}\")\n",
    "\n",
    "# Verification: 3 * (2.0)^2 = 3 * 4 = 12\n",
    "assert x.grad == 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This `requires_grad=True` and `.backward()` is the heart of training.\n",
    "When we build a Neural Network:\n",
    "1.  **Weights** are tensors with `requires_grad=True`.\n",
    "2.  **Loss** is the result of the equation ($y$).\n",
    "3.  We call `loss.backward()` to find out how to adjust the weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
