{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Expert: The Missing Manual\n",
    "\n",
    "This is not a tutorial. This is a **Lab Bench**.\n",
    "We are going to break things to understand how PyTorch *really* works.\n",
    "\n",
    "1.  **Autograd Mechanics:** In-place operations, Leaf nodes, and Gradient Accumulation.\n",
    "2.  **Module Internals:** Buffers (`register_buffer`), Hooks, and State Dicts.\n",
    "3.  **Optimization:** `inference_mode` vs `no_grad`, and `torch.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Automagic Gradient (Autograd)\n",
    "\n",
    "### Experiment 1.1: The \"In-Place\" Trap\n",
    "Why does `x += 1` sometimes crash your training?\n",
    "- `y = x + 1`: Creates a **NEW** tensor. Safe.\n",
    "- `x += 1`: Modifies data **IN MEMORY**. \n",
    "\n",
    "If PyTorch needs the *original* value of `x` to calculate gradients later, but you overwrote it... **BOOM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¥ CAUGHT ERROR: a leaf Variable that requires grad is being used in an in-place operation.\n"
     ]
    }
   ],
   "source": [
    "# SETUP: A simple operation y = x * w\n",
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = x * w\n",
    "\n",
    "# IN-PLACE OPERATION (The Sabotage)\n",
    "# We modify 'w' in place. \n",
    "# But 'y' needs 'w' to calculate gradient for 'x' (dy/dx = w).\n",
    "try:\n",
    "    w += 1 \n",
    "    y.backward()\n",
    "    print(\"Success? No.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"ðŸ’¥ CAUGHT ERROR: {e}\")\n",
    "\n",
    "# Takeaway: Avoid in-place ops (+=, *=) on tensors that require grad, unless you know exactly what you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.2: Gradient Accumulation (Simulating Big Batches)\n",
    "GPU out of memory? Batch size too small? \n",
    "**Solution:** Don't call `optimizer.zero_grad()` every step.\n",
    "Gradients **ACCUMULATE** (add up) by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weight Grad: None\n",
      "Grad after Step 1: 1.6357\n",
      "Grad after Step 2 (Accumulated): -0.4097\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(10, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Fake data\n",
    "inputs = torch.randn(4, 10)\n",
    "targets = torch.randn(4, 1)\n",
    "\n",
    "# Standard Loop (Batch Size = 4)\n",
    "print(f\"Initial Weight Grad: {model.weight.grad}\")\n",
    "\n",
    "# Step 1\n",
    "output = model(inputs[0])\n",
    "loss = (output - targets[0])**2\n",
    "loss.backward()\n",
    "print(f\"Grad after Step 1: {model.weight.grad[0][0]:.4f}\")\n",
    "\n",
    "# Step 2 (NO ZERO_GRAD)\n",
    "output = model(inputs[1])\n",
    "loss = (output - targets[1])**2\n",
    "loss.backward()\n",
    "print(f\"Grad after Step 2 (Accumulated): {model.weight.grad[0][0]:.4f}\")\n",
    "\n",
    "# NOW we step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Module Internals\n",
    "\n",
    "### Experiment 2.1: Buffers (`register_buffer`)\n",
    "Not all numbers in a model are Weights.\n",
    "Example: `BatchNorm` tracks \"Running Mean\". It needs to be saved (`state_dict`), but it **does not** need Gradient Descent.\n",
    "We use `register_buffer` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: [Parameter containing:\n",
      "tensor([1.5884], requires_grad=True)]\n",
      "State Dict Keys: odict_keys(['weight', 'running_count'])\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Parameter: Learned by Optimizer\n",
    "        self.weight = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "        # Buffer: Saved, but NOT learned\n",
    "        self.register_buffer('running_count', torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.running_count += 1 # We manually update this\n",
    "        return x * self.weight\n",
    "\n",
    "m = MyModule()\n",
    "print(f\"Parameters: {list(m.parameters())}\") # Only weight shows up\n",
    "\n",
    "# But it is in the state_dict!\n",
    "print(f\"State Dict Keys: {m.state_dict().keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.2: Forward Hooks (The Debugger)\n",
    "Need to see the shape of a tensor inside a ResNet layer? Don't use `print()`. Use a HOOK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOOK -> Input Shape: torch.Size([2, 10])\n",
      "HOOK -> Output Shape: torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(10, 5)\n",
    "\n",
    "def print_shape_hook(module, input, output):\n",
    "    # Input is a tuple\n",
    "    print(f\"HOOK -> Input Shape: {input[0].shape}\")\n",
    "    print(f\"HOOK -> Output Shape: {output.shape}\")\n",
    "\n",
    "# Register hook\n",
    "handle = model.register_forward_hook(print_shape_hook)\n",
    "\n",
    "# Run model\n",
    "x = torch.randn(2, 10)\n",
    "y = model(x)\n",
    "\n",
    "# Cleanup (Important!)\n",
    "handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Optimization & Speed\n",
    "\n",
    "### Experiment 3.1: `no_grad` vs `inference_mode`\n",
    "- `torch.no_grad()`: Disables Gradient Calculation. Standard for validation.\n",
    "- `torch.inference_mode()`: Disables Gradients AND View-Tracking. **Faster**. Use this for production/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.1 Âµs Â± 2.88 Âµs per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n",
      "7.57 Âµs Â± 221 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n",
      "8.75 Âµs Â± 1.96 Âµs per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 32, requires_grad=True)\n",
    "w = torch.randn(32, 32, requires_grad=True)\n",
    "\n",
    "%timeit x @ w\n",
    "with torch.no_grad(): \n",
    "    %timeit x @ w\n",
    "with torch.inference_mode(): \n",
    "    %timeit x @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3.2: `torch.compile` (PyTorch 2.0)\n",
    "This turns your Python code into optimized Graph code (Triton/C++).\n",
    "It takes a moment to \"Compile\" the first time, then it flies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "Done.\n",
      "Optimized Run Time: 0.5288s\n"
     ]
    }
   ],
   "source": [
    "def foo(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "# The new magic line\n",
    "opt_foo = torch.compile(foo)\n",
    "\n",
    "x = torch.randn(1000, 1000)\n",
    "y = torch.randn(1000, 1000)\n",
    "\n",
    "# First run (Compiles - might be slow)\n",
    "print(\"Compiling...\")\n",
    "opt_foo(x, y)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Subsequent runs are optimized\n",
    "import time\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    opt_foo(x, y)\n",
    "print(f\"Optimized Run Time: {time.time() - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: The Data Pipeline (The Bottleneck)\n",
    "\n",
    "### Experiment 4.1: `collate_fn` (Handling Messy Data)\n",
    "Default DataLoader expects every tensor to be the same size (to stack them).\n",
    "What if you have audio/text of different lengths?\n",
    "- **FAIL:** Default collate throws error.\n",
    "- **FIX:** Custom `collate_fn` to pad them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¥ CRASH: stack expects each tensor to be equal size, but got [3] at entry 0 and [2] at entry 1\n",
      "\n",
      "With Custom Collate:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 0]])\n",
      "tensor([[6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Fake Dataset returning variable length tensors\n",
    "dataset = [\n",
    "    torch.tensor([1, 2, 3]),\n",
    "    torch.tensor([4, 5]),\n",
    "    torch.tensor([6, 7, 8, 9])\n",
    "]\n",
    "\n",
    "# Try default loader\n",
    "try:\n",
    "    loader = DataLoader(dataset, batch_size=2)\n",
    "    for batch in loader:\n",
    "        print(batch)\n",
    "except Exception as e:\n",
    "    print(f\"ðŸ’¥ CRASH: {e}\")\n",
    "\n",
    "# DEFINE CUSTOM COLLATE\n",
    "def padding_collate(batch):\n",
    "    # batch is a list of tensors: [tensor([1,2,3]), tensor([4,5])]\n",
    "    # We pad them to match the longest one\n",
    "    return pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "\n",
    "print(\"\\nWith Custom Collate:\")\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=padding_collate)\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    # Now they are stacked nicely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4.2: Speed (`pin_memory` & `num_workers`)\n",
    "- `num_workers > 0`: Uses multiprocessing to load data (CPU) while GPU is training. Essential.\n",
    "- `pin_memory=True`: Allocates staging memory so transfer to GPU is faster.\n",
    "\n",
    "**Rule of Thumb:**\n",
    "```python\n",
    "DataLoader(dataset, batch_size=32, num_workers=4, pin_memory=True)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
